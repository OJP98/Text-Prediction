{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598123168851",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio #3 - Predicción de textos\n",
    "\n",
    "* Oscar Juárez - 17315\n",
    "* José Pablo Cifuentes - 17509\n",
    "* Paul Belches - 17088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Definirmos lista de stopwords según nltk\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Para el modelo\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación y limpieza de datos\n",
    "\n",
    "### 1. Abrir y leer archivos.\n",
    "\n",
    "Cabe mencionar que todos los archivos fueron convertidos a minúsculas, se quitan los urls y en algunas ocasiones, la mayoría de símbolos que consideramos innecesarios. También se quitan las stopwords, los números y finalmente las apostrophes. Además, se separan oraciones mediante los símbolos de **.**, **!** y **?**. Se debe validar que no hayan espacios vacíos luego de estas oraciones. \n",
    "\n",
    "#### Caso 1: Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se instancian arreglos\n",
    "blog = []\n",
    "\n",
    "with open('./files/en_US.blogs.txt', 'r', encoding='utf-8') as blog_txt:\n",
    "    for line in blog_txt:\n",
    "        # Quitar saltos de linea y pasar todo a minusculas\n",
    "        line = line.rstrip('\\n').lower()\n",
    "        # Quitar URLS\n",
    "        line = re.sub(r'^https?:\\/\\/.[\\r\\n]', '', line)\n",
    "        # Quitar el resto de expresiones regulares, excepto . ? ! y '\n",
    "        line = re.sub(r\"[^\\w.?!\\d'\\s]\", '', line)\n",
    "        # Quitar números\n",
    "        line = re.sub(r'[0-9]', ' ', line)\n",
    "        # Quitar espacios extra\n",
    "        line = line.strip(' \\t\\n\\r')\n",
    "        # Quitamos todas las stopwords\n",
    "        line = [word for word in line.split(' ') if word not in stopwords]\n",
    "        line = ' '.join(line)\n",
    "        #Finalmente, quitamos apostrofes\n",
    "        line = line.replace(\"'\", '')\n",
    "        # Separar posibles oraciones\n",
    "        dotSentences = line.split('.')\n",
    "        excSentences = line.split('!')\n",
    "        queSentences = line.split('?')\n",
    "\n",
    "        # Validar y verificar que valga la pena recorrer varias oraciones\n",
    "        if len(dotSentences) > 1:\n",
    "            for sentence in dotSentences:\n",
    "                # Por cada posible oración, debemos quitar los símbolos de puntuación\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence).strip()\n",
    "                if len(sentence) > 1: blog.append(sentence)\n",
    "\n",
    "        elif len(excSentences) > 1:\n",
    "            for sentence in excSentences:\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "                if len(sentence) > 1: blog.append(sentence)\n",
    "        \n",
    "        elif len(queSentences) > 1:\n",
    "            for sentence in queSentences:\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "                if len(sentence) > 1: blog.append(sentence)\n",
    "\n",
    "        elif len(line.split(' ')) > 1:\n",
    "                line = re.sub(r'[^\\w]', ' ', line).strip()\n",
    "                blog.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso 2: Noticias\n",
    "\n",
    "Este caso tuvo un procedimiento igual al caso 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news = []\n",
    "\n",
    "with open('./files/en_US.news.txt', 'r', encoding='utf-8') as news_txt:\n",
    "    for line in news_txt:\n",
    "        # Quitar saltos de linea y pasar todo a minusculas\n",
    "        line = line.rstrip('\\n').lower()\n",
    "        # Quitar URLS\n",
    "        line = re.sub(r'^https?:\\/\\/.[\\r\\n]', '', line)\n",
    "        # Quitar el resto de expresiones regulares, excepto . ? ! y '\n",
    "        line = re.sub(r\"[^\\w.?!\\d'\\s]\", '', line)\n",
    "        # Quitar números\n",
    "        line = re.sub(r'[0-9]', ' ', line)\n",
    "        # Quitar espacios extra\n",
    "        line = line.strip(' \\t\\n\\r')\n",
    "        # Quitamos todas las stopwords\n",
    "        line = [word for word in line.split(' ') if word not in stopwords]\n",
    "        line = ' '.join(line)\n",
    "        #Finalmente, quitamos apostrofes\n",
    "        line = line.replace(\"'\", '')\n",
    "        # Separar posibles oraciones\n",
    "        dotSentences = line.split('.')\n",
    "        excSentences = line.split('!')\n",
    "        queSentences = line.split('?')\n",
    "\n",
    "        # Validar y verificar que valga la pena recorrer varias oraciones\n",
    "        if len(dotSentences) > 1:\n",
    "            for sentence in dotSentences:\n",
    "                # Por cada posible oración, debemos quitar los símbolos de puntuación\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence).strip()\n",
    "                if len(sentence) > 1: news.append(sentence)\n",
    "\n",
    "        elif len(excSentences) > 1:\n",
    "            for sentence in excSentences:\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "                if len(sentence) > 1: news.append(sentence)\n",
    "        \n",
    "        elif len(queSentences) > 1:\n",
    "            for sentence in queSentences:\n",
    "                sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "                if len(sentence) > 1: news.append(sentence)\n",
    "\n",
    "        elif len(line.split(' ')) > 1:\n",
    "            line = re.sub(r'[^\\w]', ' ', line).strip()\n",
    "            news.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso 3: Twitter\n",
    "\n",
    "En este caso, se toma cada distinto tweet como una oración. Es necesario quitar emojis y símbolos como #, $, %, !, @, etc. Además, se quitan urls y se permiten los símbolos: **.** **,** **'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open('./files/en_US.twitter.txt', 'r', encoding='utf-8') as twitter_txt:\n",
    "    for line in twitter_txt:\n",
    "        # Quitar \\n y pasarlo a minusculas\n",
    "        line = line.replace('\\n', '').lower()\n",
    "        # Quitar URLS\n",
    "        line = re.sub(r'^https?:\\/\\/.[\\r\\n]', '', line)\n",
    "        # Quitar el resto de expresiones regulares, excepto . , y '\n",
    "        line = re.sub(r\"[^\\w.,\\d'\\s]\", '', line)\n",
    "        # Quitar números fuera de contexto\n",
    "        line = re.sub('^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', line)\n",
    "        # Añadirlos a la lista de tweets\n",
    "        tweets.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = blog + news + tweets\n",
    "random.shuffle(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_size = int(len(complete_data)*0.005)\n",
    "print('Se va a utilizar ' + str(data_size) + ' datos')\n",
    "data = complete_data[:data_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear CSV con las palabras utilizadas para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"oraciones\"])\n",
    "df.to_csv('training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un tokenizer lo cual es una representacion de enteros de cada palabra en nuestra data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el largo de nuestro vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapeamos 2 palabras a una palabra\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "    sequence = encoded[i-2:i+1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separamos en los elementos inputs y outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenaoms el modelo\n",
    "model.fit(X, y, epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('deep_no_stopwords')"
   ]
  }
 ]
}